\section{Future Work}

The work in this thesis has led to the identification of a number of areas 
which are of particular interest for expanding this research. Although 
mentioned where appropriate in each section, here we will collect all of the 
avenues and elaborate on each.

While the results of the Participant Performance Experiment aligned with our 
hypothesis, the inability to perform a statistical significance test means we 
cannot definitively confirm that individual participant performance correlates 
directly to \tvt accuracy. Additionally, because this experiment requires 
segregating participants into separate groups, the \tvt accuracy is negatively 
affected. It would be valuable to explore alternative ways of identifying 
whether or not there is a statistically significant relationship between 
participant task accuracy and \tvt accuracy with another experiment.

In the Time Windowing Experiment results we identified a statistically 
significant spike in \tvt accuracy in an earlier time window that we did not 
expect to see. After further research, we identified a number of cases in other 
work where semantics have been identified in similarly earlier periods, 
including in the similar experiment by Sudre et al. using MEG~\cite{Sudre2012}.
It would be valuable to explore what leads to this early identification of 
statistically significant accuracy, and whether or not it contributes to 
similar features of the semantic word vectors when compared against the later 
peak in accuracy.

In a similar fashion, it would be interesting to explore in general what 
sections of the cortex are contributing to what features of the semantic word 
vectors. As the earlier research in MEG used more simple, human created word 
vectors they were able to identify which areas of the brain led to highly 
predicted values of each word vector index. The Skip-Gram word vectors are not 
as directly interpretable, but methods such as Principle Component Analysis 
(PCA) exist which use linear transformations to reduce the dimensionality of 
large vectors down to a smaller set of new variables which may be more 
interpretable~\cite{jolliffe2016principal}. If an interpretable dimension is 
found, inverse PCA could potentially be used to find a vector whose values may 
be used to combine the individual weights of the corresponding models for each 
dimension of the vector. This would produce a weight map matching the brain at 
each time stamp, which may yield interpretable results, but further work is 
required to explore this.

Another interesting aspect would be the comparison of words on a more 
individual level. Does the model tend to confuse some words more than others?  
While we hypothesize this is likely, it would be valuable to determine exactly 
which words and how semantically similar those words are.

Lastly, this research focuses on the translation of single words and on the 
application of this methodology to learning, but much work has been done in 
fMRI and MEG using sentences, adjective-noun pairs, and other more complex 
components of speech. With this baseline evidence that EEG is an effective 
means for studying semantic representation in the brain, it would be valuable 
to continue to replicate these experiments in EEG and further explore what 
benefits can be provided by this collection modality.

To work toward these goals we have initiated a few projects of interest. Our 
first project is a similar study utilizing EEG, but with a focus on English 
words. In this experiment we've chosen the original 60 words used by Mitchell 
et al.~\cite{Mitchell2008} with a larger and consistent number of repititions 
per word for a larger number of participants.  We anticipate this may help us 
continue to explore the adaption of these experiments to EEG in a more close 
replication of the earlier studies.

We've also initially explored the study of more cost effective EEG devices, 
such as the UltraCortex (Markv IV, OpenBCI, New York, USA). These style of 
devices are substantially cheaper and should much allow larger quantities of 
data collection, but consequently have the drawback that the data is poorer 
quality with less sensors.  We hope that this approach will show some 
attributes of semantics remain identifiable by offsetting the more noisy signal 
with larger quantities of data.
