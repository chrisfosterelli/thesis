\subsection{Data Preprocessing}
\label{sec:preprocessing}
EEG data generally contains artifacts that must be identified and corrected for. For example, artifacts are generated by the electrical activity of the muscular movements associated with eye blinks or even eye movements. Other movements, such as a turning the head, will also generate artifacts. Further, movement over time can cause an individual electrode's connection with the scalp to be compromised which results in an excessively noisey or completely flat signal for that channel. These are natural products of collecting EEG data. 

To correct for these we adjusted the data from each participant using the Brain Vision Analyzer (Version 2.1.1, Brain Products GmbH, Munich, Germany) software suite. We visually inspected the channel streams of participants to identify flat channels or channels with bad signal. These low-quality channels were marked and removed from the dataset, and later reintroduced using interpolation via spherical splines. This process ensures that all subjects have similar data shapes for the model to process. To reduce the size of the data we then downsampled the signal to 250Hz from the original 500Hz. We also re-referenced from the original reference electrode to the average mastoid reference for improved resillience to general noise and ran a dual pass phase free Butterworth filter (pass band: 0.1 Hz to 30 Hz; notch filter: 60 Hz) to remove environmental and electrical noise.

We converted the data from EEG stream information to epochs by extracting the -1000 ms to 2000 ms window around each symbol onset event. We used a large time range initially to improve our ability to correct for eye blinks and movement artifacts. The identification for those repetitive artifacts was done using independent component analysis (ICA)~\cite{luck2014introduction}, specifically a restricted fast ICA with classic PCA sphering. This process continued until either a convergence bound of 1.0 x 10-7 or 150 steps had been reached. We manually inspected the component head maps and related factor loading to identify ocular artifacts and corrected for these using ICA back transformation. 

We then re-segmented the data to epochs witha a smaller 1000ms window following stimulus onset, which is the time length used for the actual models. The EEG signal can periodically drift over time, which may make it difficult to compare similar stimuli across exposures, so we performed baseline correction for this using the 200ms prior to the stimulus onset. 

While these methods are effective for reducing noise and artifacts in the EEG data, some events may make the data too unusable even after these corrections. For example, if a subject sneezes there is little correction that can be done to improve the signal. Therefore, these cases must be identified and removed from the dataset so they do not confuse the models. This process is called artifact rejection. The artifact rejection utility analyzes every channel on every exposure and removes the exposure if it either contains an absolute difference between the lowest and highest voltage of more than $100{\mu}V$ on that channel, or if the increase between any two samples on any channel for any exposure was more than $10{\mu}V/ms$. 
