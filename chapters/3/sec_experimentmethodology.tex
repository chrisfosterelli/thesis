\section{Experiment Methodology}
\label{sec:methodology}

\subsection{Overview}
The experiment methodology follows Sudre et al.~\cite{Sudre2012} using the {\bf 2 vs 2 test}. We train a series of machine learning ridge regression models using the EEG data to generate the input features and individual indexes of {\bf word vectors} matching our word set as the models' predictions.

\subsection{Word Vectors}
We use the {\bf Skip-Gram} word vector set from Mikolov et al.~\cite{Mikolov2013}. These word vectors are generated by a neural network with a single hidden layer containing 300 neurons. The neural network is trained to perform a word collocation task: the network receives a single word as input and predicts the probable collocated words for that input word. Pairs of words are generated from the Google News text corpus using a sliding window over the text corpus. After training, the weights of the model can approximate the probability of collocated words. Each word has associated weights, which create 300-dimensional vectors to use as training data in our experiment.

 Skip-Gram word vectors are a reasonable proxy for word semantics. Hollis et al. showed that Skip-Gram could predict human judgments for semantic tasks (e.g. sentiment ratings)~\cite{hollis2017extrapolating}. Hill et al. additionally concluded that Skip-Gram performs well on their SlimLex-999 evaluation, a high quality word similarity benchmark for computational models of word meaning~\cite{hill2016simlex}. Further, Murphy et al. showed that computational models can perform similarly to human benchmarks in the specific context of neurolinguistic decoding tasks~\cite{Murphy2012}, and subsequent work showed specifically that Skip-Gram could be used to identify the semantics of many word types in fMRI, EEG, and MEG~\cite{xu2016brainbench}. The semantic properties of these word vectors make them a useful tool for performing semantic analysis on brain data.

\subsection{Prediction Model}
After the data preprocessing steps mentioned in Section \ref{sec:preprocessing}, every participant-symbol pair is represented by a tensor $D \in \mathbb{R}^{(r \times n_e \times l)}$, where $r$ can be between $0 \ldots n_t$, $n_t$ is the maximum number of possible trials seen for a given symbol, $n_e$ is the total number of electrodes, and $l$ is the number of time steps. Due to the randomness of the paradigm, $r$ varies across $D$s. Each trial is a matrix in $D$ with dimension $n_e \times l$. Further, we use $n_p$ to denote the number of participants and $n_s$ to denote the number of symbols. 

Depending on the type of analysis being performed, we select some trials from the set of all $D$. The selection process may choose all $D$ for certain participants or choose certain trials from each $D$ (see Section \ref{sec:results} and Section \ref{sec:discussion} for more details). We average across all participants and trials to create a tensor of dimension $n_s \times n_e \times l$, denoted as $D_\text{selected}$.  

This selection and averaging process is shown in Figure~\ref{fig:selection}.

\begin{figure}[t]
 \centerline{
   \includegraphics[width=\linewidth]{figures/selection}
 }
 \caption{The trial selection pipeline. Our initial data contains participant-word pairs $(s, w)$ for $n_p$ participants and $n_s$ words that each contain between 0 and $n_t$ trials. The trials are of length $l$ and are recorded with $n_e$ electrodes. We select some subsection of these trials, and then average the data across participants to generate $(s, w)_{\text{selected}}$ which contains the averaged trials for each word.}
 \label{fig:selection}
\end{figure}

Before we train regression models, $D_{\text{selected}}$ is reshaped to produce a matrix with dimensions $X \in \mathbb{R}^{n_s \times (n_e * l)}$.  With a sampling rate of 250Hz for a 700ms window with 61 data sensors there will be $61*175 = 10675$ numerical features. We train $v$ independent regression models, such that we have one model to predict each dimension of the Skip-Gram word vector set. We use a linear least squares loss function and l2-norm regularization (ridge regression):

\begin{equation}
  \underset{w_i}{min\,} {|| X w_i - y_i||_2^2 + \alpha ||w_i||_2^2}
  \label{eq:ridge}
\end{equation}

\noindent where regression model $i$ is trained to predict the $i$th dimension of the word vectors (vector $y_i$) using weights $w_i$. $\alpha$ is a hyperparameter that controls the level of regularization. We use a standard $\alpha = 0.1$, although several values were tried empirically and found the only minor variation in performance. Using a trained regression model we can predict a single element of a word vector for a given input via $\hat{y}_i = x \cdot w_i$.

The weights of the individual models can be concatenated such that $W = [ w_1, w_2, ... w_v ]$. Collectively, $W$ is a single model that produces a predicted word vector using $\hat{y} = xW$. An example evaluation of the model on a single input vector $x$ from $X$ is seen in Figure ~\ref{fig:features}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/features}
  \caption{An evaluation of the trained model that predicts a word vector from EEG data. The set of regression models can be viewed collectively as a single model that takes a single averaged EEG trial $x$ as input and predicts a word vector $\hat{y}$ as output. $W$ is the learned weights from the regression models. The number of EEG features is $n$, which varies depending on the experiment, and $v$ is the length of the word vectors (for our experiments, $v=300$).}
  \label{fig:features}
\end{figure}
