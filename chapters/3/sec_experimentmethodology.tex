\section{Experiment Methodology}
\label{sec:methodology}

\subsection{Overview}
The experiment methodology follows Sudre et al.~\cite{Sudre2012} using the {\bf 2 vs 2 test}. We train a series of machine learning ridge regression models using the EEG data to generate the input features and individual indexes of {\bf word vectors} matching our word set as the models' predictions.

\subsection{Word Vectors}
We use the {\bf Skip-Gram} word vector set from Mikolov et al.~\cite{Mikolov2013}. These word vectors are generated by a neural network with a single hidden layer containing 300 neurons. The neural network is trained to perform a word collocation task: the network receives a single word as input and predicts the probable collocated words for that input word. Pairs of words are generated from the Google News text corpus using a sliding window over the text corpus. After training, the weights of the model can approximate the probability of collocated words. Each word has associated weights, which create 300-dimensional vectors to use as training data in our experiment.

 Skip-Gram word vectors are a reasonable proxy for word semantics. Hollis et al. showed that Skip-Gram could predict human judgments for semantic tasks (e.g. sentiment ratings)~\cite{hollis2017extrapolating}. Hill et al. additionally concluded that Skip-Gram performs well on their SlimLex-999 evaluation, a high quality word similarity benchmark for computational models of word meaning~\cite{hill2016simlex}. Further, Murphy et al. showed that computational models can perform similarly to human benchmarks in the specific context of neurolinguistic decoding tasks~\cite{Murphy2012}, and subsequent work showed specifically that Skip-Gram could be used to identify the semantics of many word types in fMRI, EEG, and MEG~\cite{xu2016brainbench}. The semantic properties of these word vectors make them a useful tool for performing semantic analysis on brain data.

\subsection{Prediction Model}
After the data preprocessing steps mentioned in Section \ref{sec:preprocessing}, every participant-symbol pair is represented by a tensor $D \in \mathbb{R}^{(r \times n_e \times l)}$, where $r$ can be between $0 \ldots n_t$, $n_t$ is the maximum number of possible trials seen for a given symbol, $n_e$ is the total number of electrodes, and $l$ is the number of time steps. Due to the randomness of the paradigm, $r$ varies across $D$s. Each trial is a matrix in $D$ with dimension $n_e \times l$. Further, we use $n_p$ to denote the number of participants and $n_s$ to denote the number of symbols. 

Depending on the type of analysis being performed, we select some trials from the set of all $D$. The selection process may choose all $D$ for certain participants or choose certain trials from each $D$ (see Section \ref{sec:results} and Section \ref{sec:discussion} for more details). We average across all participants and trials to create a tensor of dimension $n_s \times n_e \times l$, denoted as $D_\text{selected}$. This selection and averaging process is shown in Figure~\ref{fig:selection}.

\begin{figure}[t]
 \centerline{
   \includegraphics[width=\linewidth]{figures/selection}
 }
 \caption{The trial selection pipeline. Our initial data contains participant-word pairs $D$ for $n_p$ participants and $n_s$ words that each contain between 0 and $n_t$ trials ($r$). The trials are of length $l$ and are recorded with $n_e$ electrodes. We select some subsection of these trials, and then average the data across participants to generate $D_{\text{selected}}$ which contains the averaged trials for each word.}
 \label{fig:selection}
\end{figure}

Before we train regression models, $D_{\text{selected}}$ is reshaped to produce a matrix with dimensions $X \in \mathbb{R}^{n_s \times (n_e * l)}$.  With a sampling rate of 250Hz for a 700ms window with 61 data sensors there will be $61*175 = 10675$ numerical features. We train $v$ independent regression models, such that we have one model to predict each dimension of the Skip-Gram word vector set. We use a linear least squares loss function and l2-norm regularization (ridge regression):

\begin{equation}
  \underset{w_i}{min\,} {|| X w_i - y_i||_2^2 + \alpha ||w_i||_2^2}
  \label{eq:ridge}
\end{equation}

\noindent where regression model $i$ is trained to predict the $i$th dimension of the word vectors (vector $y_i$) using weights $w_i$. $\alpha$ is a hyperparameter that controls the level of regularization. We use a standard $\alpha = 0.1$, although several values were tried empirically and found the only minor variation in performance. Using a trained regression model we can predict a single element of a word vector for a given input via $\hat{y}_i = x \cdot w_i$.

The weights of the individual models can be concatenated such that $W = [ w_1, w_2, ... w_v ]$. Collectively, $W$ is a single model that produces a predicted word vector using $\hat{y} = xW$. An example evaluation of the model on a single input vector $x$ from $X$ is seen in Figure ~\ref{fig:features}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/features}
  \caption{An evaluation of the trained model that predicts a word vector from EEG data. The set of regression models can be viewed collectively as a single model that takes a single averaged EEG trial $x$ as input and predicts a word vector $\hat{y}$ as output. $W$ is the learned weights from the regression models. The number of EEG features is $n$, which varies depending on the experiment, and $v$ is the length of the word vectors (for our experiments, $v=300$).}
  \label{fig:features}
\end{figure}

\subsection{Evaluation Framework}
The set of ridge regression models are then evaluated in a ``leave two out'' fashion. That is, we hold out pairs of symbols $(y_i, y_j)$, and train ridge regression models to predict the vectors of the associated words using the EEG data from the remaining $n_s-2$ symbols.  The trained model is used to predict the two target word vectors $\hat{y}_i$ and $\hat{y}_j$ from the held out EEG data. The true word vectors ($y_i$, $y_j$) are then compared to the predicted word vectors ($\hat{y}_i$, $\hat{y}_j$) using a vector distance metric $d$ (in our case the cosine distance). The prediction is considered successful if the sum of the distances between the correctly matched true and predicted word vectors is smaller than the distance of the mismatched vectors as in: 
  
\begin{equation}
  d(y_i, \hat{y}_i) + d(y_j, \hat{y}_j) < d(y_i, \hat{y}_j) + d(y_j, \hat{y}_i)
  \label{eq:2vs2}
\end{equation}
  
\noindent We run this test for all possible ${\binom{n_s}{2}}$ pairs of words. The \tvt test can detect if the EEG data is correlated with the word vectors. If the EEG data is not correlated to the word vectors, the 2 vs 2 accuracy (the percentage of the ${\binom{n_s}{2}}$ \tvt tests correct) will be  near the chance value of 50\%.

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/2vs2}
  \caption{A 2 vs. 2 test}
  \label{fig:2vs2}
\end{figure}

\subsection{Validating Statistical Significance}
We tested the statistical significance of our results from experiments in Section~\ref{sec:experiments} using permutation tests. For each experiment, we reran the pipeline, but randomly shuffled the order of the word vectors so that the true word vectors no longer correctly matched with the EEG data for each symbol. This randomization was done after averaging over participants and symbols. We ran the same experiments on 300 permutations of the data, and used the resulting 300 \tvt accuracies to approximate the null distribution (where the data and labels have no statistical relationship). 
  
As expected, we found that the empirical null distribution had a mean close to 50\% (chance accuracy) for all experiments. The $p$-values were obtained by testing the reported accuracy against a Gaussian kernel density estimation fit to the associated empirical null distribution. We corrected for multiple testing using the Benjamini-Hochberg-Yekutieli procedure where applicable~\cite{benjamini2001control}, with an alpha value of 0.05.
