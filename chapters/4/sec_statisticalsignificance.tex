\section{Validating Statistical Significance}
We tested the statistical significance of our results from experiments in 
Chapter~\ref{chapter:experiments} using permutation tests. For each experiment, 
we reran the pipeline, but randomly shuffled the order of the word vectors so 
that the true word vectors no longer correctly matched with the EEG data for 
each symbol. This randomization was done after averaging over participants and 
symbols. We ran the same experiments on 300 permutations of the data, and used 
the resulting 300 \tvt accuracies to approximate the null distribution (where 
the data and labels have no statistical relationship). 

As expected, we found that the empirical null distributions across all 
experiments had an average mean of 50.01\% (chance accuracy) with $\sigma = 
0.4\%$. The $p$-values were obtained by testing the reported accuracy against a 
Gaussian kernel density estimation fit to the associated empirical null 
distribution. We corrected for multiple testing using the 
Benjamini-Hochberg-Yekutieli procedure where 
applicable~\cite{benjamini2001control}, with an alpha value of 0.05.

We also utilized bootstrap testing, which allowed us to compute a statistical 
significance score for the difference in \tvt accuracies across two 
experiments. To perform bootstrapping, we sampled with replacement $n_p$ times 
from the list of participants and reran the model pipeline on those sampled 
participants with the parameters of each experiment. This is repeated $R$ 
times.  Similar to the permutation test, the resulting \tvt accuracies form 
empirical distributions for each experiment. We generate a normal-theory 
confidence interval around the real \tvt scores using the respective empricial 
distribution. We compare two of these confidence intervals, one for each 
experiment, to test if two \tvt scores for an experiment are statistically 
different. In this work here we use $R = 100$ and generate the confidence 
intervals with $p < 0.05$.
