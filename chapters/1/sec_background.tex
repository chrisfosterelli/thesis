\section{Background and Terminology}

This section will briefly introduce the relevant terminology at a high level, 
for those not familiar with the fields of neuroscience or machine learning.  
With a cursory understanding of these, the topics discussed in the thesis 
should be easier to follow. This is not designed to be a complete review of the 
topics and readers are encouraged to investigate further if a topic is 
unfamiliar to them.

Machine learning is a subfield of artificial intelligence that gives computers 
the ability to improve their performance at a task in response to data about 
that task (called training data). This is done using statistical techniques.  
The most common type of machine learning is supervised learning, in which 
training data consists of pairs of inputs and desired outputs. The algorithm 
learns to map from a given input $x$ to a predicted output $\hat{y}$ using 
training data sets with inputs $X$ and matching outputs $Y$.

In this work we use linear ridge regression, a type of machine learning 
algorithm. Regression indicates that the model predicts a scalar value, rather 
than a category. We use sets of these to make a multi-output prediction, known 
as a multivariate linear regression. A linear model is a model which learns a 
polynomial function with a degree of at most one (that is, it predicts in a 
straight line). Ridge regression, also known as weight decay, is a type of 
linear regression that utilizes a regularization mechanism. This is designed to 
help the algorithm perform better on results which are not in the training set 
(known as generalization). A linear model takes a set of inputs $x_1, x_2, ...  
x_n$ and predicts an output $\hat{y}$ by multiplying weights $w_1, w_2, ...  
w_n$ with the inputs and adding the components together. The weights for a 
linear model can be estimated in a few ways, which are outside of the scope of 
this summary.

Another topic that is discussed are artificial neural networks. These are 
computational systems which are vaguely inspired by the connections in the 
brain. A \emph{neuron} is a single unit which receives inputs $x$ and applies 
weights $w$ to each input respectively (similar to linear models). However, 
after summing the components a special function known as an activation function 
is applied to the result. The result of the activation function is the output 
of the neural. The activation function is typically a non-linear function, 
which allows the artificial neural network to learn to model non-linear data.  
As the phrase "network" suggests, these neurons are generally connected in 
series and parallel to form multiple layers of computation. The weights for a 
neural network are found using a process known as \emph{gradient descent}. 

Some neuroscience terminology is utilized here as well. The \emph{cortex} 
refers to the outermost layer of an organ in the body. In all cases here, we 
are referring to the cerebral cortex, which is the outermost layer of the 
cerebrum. The cerebrum is the upper, largest section of the human brain which 
is associated with higher brain operations such as speech, movement, sensory 
processing, and other functions. Much of this functionality resides directly on 
the cortex. The cortex is categorized into four lobes: the frontal, parietal, 
temporal, and occipital lobes. The cortex contains folds, which increase the 
surface area. Each fold contains a gyrus (the ridge or peak of the fold) and 
sulcus (the depression between one fold and the neighboring fold).

Electrophysiology is the study of electrical activity in biological tissues. In 
neuroscience, the electrophysiology signals of interest are the electrical 
signals from the nervous system of the body.

As mentioned prior, semantics refers to the study of meaning. This research 
area explores how the brain represents these semantics. When we discuss the 
semantic representation of a word in the brain, we are referring to the 
electrophysiological state of the brain while it is processing the meaning of a 
given word.
