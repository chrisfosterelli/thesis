\section{Generalizable Semantic Models}

Until 2008, most research utilized models that required many repeated training 
examples of a stimulus before it could correctly identify that stimulus in the 
future~\cite{kutas1980reading,kuperberg2007neural,Wang2002,Mitchell2002,Shinkareva2008,Gu2014}.
In effect, this means that these models are only capable of recognizing brain 
states that the machine learning algorithm had already been trained to 
recognize. As neuroscience training data is very expensive to collect compared 
to other applications of machine learning, this could be viewed as a limitation 
of the semantic models. It would be impractical to collect sufficient trials of 
every possible word in the English language for each subject.

By training a machine learning model to accurately predict the expected fMRI 
activity of a participant reading concrete nouns, Mitchell et al. showed that 
the semantic features of a word are correlated with fMRI data of a participant 
viewing the word~\cite{Mitchell2008}. Although the model is trained using 
observed fMRI data of participants reading 60 concrete nouns, the model is 
capable of generating predictions for thousands of words for which it has never 
seen fMRI data. This is achieved by encoding each word as a vector of 
intermediate features based on the co-occurrences of the word with 25 verbs in 
a large text corpus. Rather than training the model to recall a given word 
categorization, this forces the weights to model the semantic patterns in the 
brain. Mitchell et al. demonstrated a direct relationship between the 
statistics of word co-occurrence and the neural activation associated with each 
word's meaning~\cite{Mitchell2008}.

Another key study in this area reproduced Mitchell et al.~\cite{Mitchell2008} 
using MEG~\cite{Sudre2012}. Sudre et al. used MEG data and word vectors to 
correctly identify concrete nouns. However, in this work, the  word vectors 
were based on human responses to semantic questions about the word (e.g. Is it 
alive?  It it bigger than a golf ball?) rather than automatically generated 
features from text corpora. The use of MEG allowed Sudre et al. to pinpoint in 
time when the semantics of a word could be detected and when the strength of 
the representation was the strongest. Subsequent work showed that, with some 
fine tuning, word vectors derived from a text corpus could be as accurate for 
predicting the word a person is reading as the behavioral vectors used in Sudre 
et al.~\cite{Murphy2012}.

While most of the work discussed here focuses on the analysis of single 
concrete nouns, recent work has been done that extends into more complicated 
language structures such as phrases or sentences~\cite{Chang2009, afyshethesis, 
pereira2018toward}. In our work, we will focus on adapting the single word 
paradigm to EEG. However, with this ground work established in EEG more 
complicated language structures become an obvious area for future 
experimentation.
